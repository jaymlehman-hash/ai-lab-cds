{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi‑Lab Reasoning Analysis Notebook\n",
    "\n",
    "This notebook analyzes the merged dataset containing:\n",
    "- Panel metadata\n",
    "- CDS outputs\n",
    "- LLM outputs\n",
    "- Scoring results\n",
    "\n",
    "It produces:\n",
    "- Summary statistics\n",
    "- Score distributions\n",
    "- CDS vs LLM comparisons\n",
    "- Drift and correlation‑detection patterns\n",
    "- Outlier detection\n",
    "- Relationship heatmaps\n",
    "\n",
    "This is the primary analysis notebook for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "DATA_PATH = \"../outputs/dataset/merged_dataset.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Summary\n",
    "\n",
    "Basic descriptive statistics across all scoring dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cols = [\n",
    "    'correctness_score', 'completeness_score', 'relationship_detection_score',\n",
    "    'relationship_accuracy_score', 'narrative_drift_score', 'certainty_score',\n",
    "    'mechanistic_score', 'structure_score', 'total_score'\n",
    "]\n",
    "\n",
    "df[score_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Distributions\n",
    "\n",
    "Visualizing how the model performs across each scoring dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "for i, col in enumerate(score_cols):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.histplot(df[col], kde=True, bins=10)\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix of Scores\n",
    "\n",
    "Shows how different reasoning dimensions relate to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df[score_cols].corr(), annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title(\"Score Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDS vs LLM Output Length\n",
    "\n",
    "A proxy for reasoning breadth and verbosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['llm_length'] = df['llm_output'].fillna('').apply(len)\n",
    "df['cds_length'] = df['cds_output'].fillna('').apply(len)\n",
    "\n",
    "sns.scatterplot(data=df, x='cds_length', y='llm_length')\n",
    "plt.xlabel(\"CDS Output Length\")\n",
    "plt.ylabel(\"LLM Output Length\")\n",
    "plt.title(\"CDS vs LLM Output Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drift vs Total Score\n",
    "\n",
    "Shows whether narrative drift correlates with overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='narrative_drift_score', y='total_score')\n",
    "plt.title(\"Narrative Drift vs Total Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship Detection vs Mechanistic Depth\n",
    "\n",
    "This highlights whether the model's ability to detect correlations is tied to deeper physiologic reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='relationship_detection_score', y='mechanistic_score')\n",
    "plt.title(\"Relationship Detection vs Mechanistic Reasoning Depth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Panels\n",
    "\n",
    "Panels with unusually high or low total scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('total_score').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('total_score').tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panel Inspection Utility\n",
    "\n",
    "A helper function to inspect a specific panel’s:\n",
    "- metadata\n",
    "- CDS output\n",
    "- LLM output\n",
    "- scoring breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_panel(panel_id):\n",
    "    row = df[df['panel_id'] == panel_id].iloc[0]\n",
    "    print(\"=== PANEL METADATA ===\")\n",
    "    display(row.filter(regex='lab|panel'))\n",
    "\n",
    "    print(\"\\n=== CDS OUTPUT ===\")\n",
    "    print(textwrap.fill(row['cds_output'] or '', width=100))\n",
    "\n",
    "    print(\"\\n=== LLM OUTPUT ===\")\n",
    "    print(textwrap.fill(row['llm_output'] or '', width=100))\n",
    "\n",
    "    print(\"\\n=== SCORING ===\")\n",
    "    display(row[score_cols])\n",
    "\n",
    "inspect_panel(\"P001\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
